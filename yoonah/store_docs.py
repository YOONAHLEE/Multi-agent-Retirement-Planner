import os
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_ollama import OllamaEmbeddings 
# from langchain.vectorstores import FAISS
from langchain_chroma import Chroma
from langchain.document_loaders import PyPDFLoader, WebBaseLoader

# from langchain.chat_models import ChatOpenAI
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
# from langchain.schema import Document
from langchain_core.documents import Document as LangchainDocument
from datetime import datetime, timedelta
from typing import TypedDict, List, Dict, Any
from langgraph.graph import END, StateGraph
from datetime import datetime, timedelta
import re
from termcolor import colored
import pandas as pd
import holidays


load_dotenv("openai.env")

# ëª¨ë¸ ì„¸íŒ…
# llm = ChatOpenAI(model="gpt-4", temperature=0.3)
llm = ChatGroq(model='llama-3.3-70b-versatile')
# embedding_model = OllamaEmbeddings(model='bge-m3')
vectorstore_path = "vectordb"
embedding_model = OpenAIEmbeddings()


def get_previous_business_day(date: datetime) -> datetime:
    """Get the previous business day, skipping weekends and Korean holidays"""
    kr_holidays = holidays.KR()
    
    while True:
        date = date - timedelta(days=1)
        # Check if it's a weekday and not a holiday
        if date.weekday() < 5 and date not in kr_holidays:
            return date

def get_target_date() -> tuple[datetime, str]:
    """Get the target date for data retrieval, considering weekends and holidays"""
    current_date = datetime.now()
    kr_holidays = holidays.KR()
    
    # Check if today is a weekend or holiday
    if current_date.weekday() >= 5 or current_date in kr_holidays:
        print(f"ì˜¤ëŠ˜ì€ {'ì£¼ë§' if current_date.weekday() >= 5 else 'ê³µíœ´ì¼'}ì…ë‹ˆë‹¤.")
        target_date = get_previous_business_day(current_date)
        print(f"ì´ì „ ì˜ì—…ì¼ì¸ {target_date.strftime('%Y-%m-%d')} ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    else:
        target_date = current_date
        print(f"ì˜¤ëŠ˜({target_date.strftime('%Y-%m-%d')})ì€ ì˜ì—…ì¼ì…ë‹ˆë‹¤.")
    
    # Format date for directory
    date_str = target_date.strftime("%y%m%d")
    return target_date, date_str

#  ì›¹ ë¬¸ì„œ ìˆ˜ì§‘ í•¨ìˆ˜
def fetch_html_content(url: str) -> str:
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    return response.text

def parse_naver_blog(url: str) -> LangchainDocument:
    html = fetch_html_content(url)
    soup = BeautifulSoup(html, "html.parser")
    text = soup.get_text()
    return LangchainDocument(page_content=text, metadata={"source": url})

def parse_naver_pdf(url: str) -> LangchainDocument:
    pdf_url = url.split("url=")[-1]
    pdf_url = requests.utils.unquote(pdf_url)
    pdf_path = "temp.pdf"
    with open(pdf_path, "wb") as f:
        f.write(requests.get(pdf_url).content)
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    return docs[0]

def get_documents_from_links(html_links):
    all_docs = []
    for url in html_links:
        print(f"Parsing {url}")
        if "pdf?url=" in url:
            all_docs.append(parse_naver_pdf(url))
        else:
            all_docs.append(parse_naver_blog(url))
    return all_docs

#   ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
# def build_vectorstore(documents):
#     splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
#     chunks = splitter.split_documents(documents)
#     vectorstore = Chroma.from_documents(chunks, embeddings_model)
    
#     # Check if the vectorstore is storing the embeddings correctly
#     if vectorstore:
#         print(f"Vectorstore created with {len(chunks)} chunks.")
#         sample_chunk = chunks[0].page_content if chunks else "No chunks available"
#         print(f"Sample chunk: {sample_chunk[:200]}...")  # Print first 200 characters of a sample chunk
#     else:
#         print("Failed to create vectorstore.")
    
#     return vectorstore

def build_vectorstore(documents, persist_dir):
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    chunks = splitter.split_documents(documents)

    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        persist_directory=persist_dir  # ì €ì¥ ìœ„ì¹˜ ì„¤ì •
    )
    try:
        vectorstore.persist()  
    except Exception as e:
        print(f"Error during persistence: {e}")
        return None
    
    print(f"âœ… Vectorstore created and saved with {len(chunks)} chunks.")
    return vectorstore

# ìƒíƒœ ì •ì˜
# class ReportState(TypedDict):
#     query: str
#     documents: List[LangchainDocument]
#     analysis: str
#     report_sections: Dict[str, str]
#     final_report: str
#     pdf_path: str

def create_date_directory(date_str):
    """ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ìƒì„±"""
    base_dir = "data"
    date_dir = os.path.join(base_dir, date_str)
    if not os.path.exists(date_dir):
        os.makedirs(date_dir)
    return date_dir

def fetch_daily_reports(target_date, date_str):
    """ë„¤ì´ë²„ ì¦ê¶Œ ë°ì¼ë¦¬ ë¦¬í¬íŠ¸ ìˆ˜ì§‘"""
    # base_urls = [
    #     "https://finance.naver.com/research/debenture_list.naver",  # ì±„ê¶Œ ë¦¬í¬íŠ¸
    #     "https://finance.naver.com/research/economy_list.naver",    # ê²½ì œ ë¦¬í¬íŠ¸
    #     "https://finance.naver.com/research/market_info_list.naver", # ì‹œì¥ì •ë³´ ë¦¬í¬íŠ¸
    #     "https://finance.naver.com/research/invest_list.naver"      # íˆ¬ì ë¦¬í¬íŠ¸
    # ]
    base_url_list = [
        "https://finance.naver.com/research/market_info_list.naver", # daily report, 
        "https://finance.naver.com/research/invest_list.naver", # íˆ¬ì ë¦¬í¬íŠ¸ 
        "https://finance.naver.com/research/economy_list.naver" # ê²½ì œ ë¦¬í¬íŠ¸ 
    ]
    date_dir = create_date_directory(date_str)

    headers = {'User-Agent': 'Mozilla/5.0'}

    for base_url in base_url_list:
        all_reports = []
        for page in range(1, 3):
                
            print(f"ğŸ” Scraping page {page}...")
            res = requests.get(f"{base_url}?page={page}", headers=headers)
            soup = BeautifulSoup(res.text, "html.parser")

            table = soup.find("table", class_="type_1")
            # print(table)
            rows = table.find_all("tr")[2:]

            for row in rows:
                cols = row.find_all("td")
                print(colored(f"cols: {cols}", "red"))

                pdf_tag = row.select_one('td.file a[href*=".pdf"]')
                print(colored(f"pdf tag: {pdf_tag}", "yellow"))
                if pdf_tag and 'href' in pdf_tag.attrs:
                    pdf_url = pdf_tag['href']
                else:
                    print("Cannot find related pdf path")
                    continue
                
                all_reports.append({
                    "title": cols[0].get_text(strip=True) if cols else "Unknown", 
                    "date": cols[3].get_text(strip=True) if len(cols) > 3 else "Unknown",
                    "pdf_url": pdf_url
                })

        # Convert to DataFrame
        df = pd.DataFrame(all_reports)
        print(df)
        
        # Set pandas display options to show full URLs
        pd.set_option('display.max_colwidth', None)  # Show full content of columns
        pd.set_option('display.width', None)  # Use full terminal width
        pd.set_option('display.max_columns', None)  # Show all columns
        
        print("\n=== Full URLs ===")
        print(df)

        
        for each in all_reports:
            if each["date"] == today:
                doc_title = re.sub(r'[^\w\-_\. ]', '_', each['title'])
                print(each['title'], doc_title)
                pdf_filename = f"{doc_title}.pdf"
                pdf_path = os.path.join(date_dir, pdf_filename)
                download_pdf(each['pdf_url'], pdf_path)
            
    

def download_pdf(url, save_path):
    """PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = requests.get(url, stream=True)
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        print(f"PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# def retrieve_documents(state: ReportState) -> ReportState:
#     print("Step 1: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
#     current_date = datetime.now()
    
#     # ì£¼ë§ì¸ ê²½ìš° ê¸ˆìš”ì¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
#     if current_date.weekday() >= 5:  # 5: Saturday, 6: Sunday
#         days_to_subtract = current_date.weekday() - 4  # ê¸ˆìš”ì¼(4)ê¹Œì§€ì˜ ì¼ìˆ˜ ê³„ì‚°
#         target_date = current_date - timedelta(days=days_to_subtract)
#         print(f"ì£¼ë§ì´ë¯€ë¡œ {target_date.strftime('%Y-%m-%d')} ê¸ˆìš”ì¼ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
#     else:
#         target_date = current_date
    
#     date_str = target_date.strftime("%y%m%d")  # YYMMDD í˜•ì‹
#     documents = process_daily_reports(date_str)
#     print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
#     return {**state, "documents": documents}


# def create_report_graph():
#     workflow = StateGraph(ReportState)
#     # ë…¸ë“œ ì¶”ê°€
#     workflow.add_node("retrieve", retrieve_documents)
#     # ì—£ì§€ ì„¤ì •
#     workflow.set_entry_point("retrieve")
#     workflow.add_edge("retrieve", END)
#     return workflow.compile()


if __name__ == "__main__":



    # ë³´ê³ ì„œ ìƒì„± ì‹œì‘
    # graph = create_report_graph()
    # query = "ìµœì‹  ì‹œì¥ ë™í–¥ê³¼ íˆ¬ì ê¸°íšŒì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
    # initial_state = ReportState({
    #     "query": query,
    #     "documents": [],
    #     "analysis": "",
    #     "report_sections": {},
    #     "final_report": "",
    #     "pdf_path": ""
    # })
    # result = graph.invoke(initial_state)

    # Today's date in 'yy.mm.dd' format (matches the Naver page format)
    # today = datetime.now().strftime('%y.%m.%d')
    
    # Get target date considering holidays
    today, date_str = get_target_date()
    today = today.strftime('%y.%m.%d')
    
    fetch_daily_reports(today, date_str)






