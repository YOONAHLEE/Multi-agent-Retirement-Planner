# advanced_financial_rag_pipeline.ipynb

# ğŸ“Œ Step 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì„¤ì •
from datetime import date
from langchain_upstage import UpstageDocumentParseLoader
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.chains.summarize import load_summarize_chain
from langchain.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings
# from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langgraph.graph import StateGraph, START, END
from langchain_core.output_parsers import StrOutputParser
import os
import glob
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate
from typing import Annotated, List, Literal, TypedDict
from store_docs import get_target_date
from langchain_core.runnables.graph_mermaid import MermaidDrawMethod
# from langchain_openrouter import ChatOpenRouter
import warnings
from termcolor import colored
from typing import Optional, ClassVar

from langchain_core.utils.utils import secret_from_env
from pydantic import Field, SecretStr


warnings.filterwarnings("ignore")
# API í‚¤ ë° ë””ë ‰í† ë¦¬ ì„¤ì •
load_dotenv("openai.env")

TODAY = date.today().isoformat()
SUMMARY_CACHE_PATH = "cache_db"

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ìš”ì•½ ë¬¸ì„œê°€ ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±í•˜ê³  ì €ì¥
class ChatOpenRouter(ChatOpenAI):
    openai_api_key: Optional[SecretStr] = Field(
        alias="api_key",
        default_factory=secret_from_env("OPENROUTER_API_KEY", default=None),
    )
    @property
    def lc_secrets(self) -> dict[str, str]:
        return {"openai_api_key": "OPENROUTER_API_KEY"}

    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 **kwargs):
        openai_api_key = (
            openai_api_key or os.environ.get("OPENROUTER_API_KEY")
        )
        super().__init__(
            base_url="https://openrouter.ai/api/v1",
            openai_api_key=openai_api_key,
            **kwargs
        )

llm = ChatOpenRouter(
    model_name="anthropic/claude-3.7-sonnet:thinking"
)

summarize_chain = load_summarize_chain(llm, chain_type="map_reduce")

### Define State
# class PipelineState:
#     def __init__(self):
#         self.date = date.today().isoformat() # 2025-05-07
#         self.last_business_day = ""
#         self.query = ""
#         self.documents = []
#         self.doc_summary = ""
#         self.web_summary = ""
#         self.final_output = ""
#         self.db_path = "cache_db"


class PipelineState(TypedDict, total=False):
    date: str
    last_business_day: str
    query: str
    documents: List[Document]
    doc_summary_chunks: List[str]
    doc_summary: str
    web_summary: str
    final_output: str
    db_path: str


### Define Nodes
def load_docs(state):
    state["date"] = date.today().isoformat()
    business_day, business_day_str = get_target_date()
    state["last_business_day"] = business_day_str
    state["db_path"] = SUMMARY_CACHE_PATH
    
    print(state)

    if not os.path.exists(state["db_path"]):
        os.makedirs(state["db_path"])
    
    # Chromaì—ì„œ í•´ë‹¹ ë‚ ì§œ ì»¬ë ‰ì…˜ ë¡œë“œ ë˜ëŠ” ìƒì„±
    vectordb = Chroma(
        collection_name=business_day_str,
        embedding_function=embeddings,
        persist_directory=state["db_path"])
    
    # ì»¬ë ‰ì…˜ì— ë¬¸ì„œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    docs = vectordb.get()
    if len(docs["ids"]) > 0 :# if collecion exists
        # load documents
        print(f"Found {len(docs['ids'])} documents in the database")
        # Convert dictionary to list of Document objects
        print("db stores documents")
        documents = [
            Document(
                page_content=doc,
                metadata=meta
            ) for doc, meta in zip(docs["documents"], docs["metadatas"])
        ]
        state["documents"] = documents

    else:
        # load documents
        path = f"data/{state['last_business_day']}"
        if not os.path.exists(path):
            from store_docs import fetch_daily_reports
            fetch_daily_reports(business_day.strftime('%y.%m.%d'), business_day_str)
        files = glob.glob(f"{path}/**.pdf", recursive=True)
        parser = UpstageDocumentParseLoader(
            files, output_format='markdown', 
            coordinates=False) 
        documents = parser.load()
        print(f"Found {len(documents)} documents to process")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„° ì œê±°
        # for doc in documents:
        #     if doc.metadata:
        #         # base64ë¡œ ì‹œì‘í•˜ëŠ” í‚¤-ê°’ ìŒ ì œê±°
        #         doc.metadata = {k: v for k, v in doc.metadata.items() 
        #                       if not isinstance(v, str) or not v.startswith('/9j/')}
        
        state["documents"] = documents
        vectordb.add_documents(documents)  # Save documents into vector DB
    return state
# ì¡°ê±´ ë¶„ê¸°: ì¿¼ë¦¬ì— "ë³´ê³ ì„œ" í¬í•¨ ì—¬ë¶€
def check_query_type(state: PipelineState) -> str:
    """
    Determine which search path to take based on query content
    """
    if "ë³´ê³ ì„œ" in state["query"]:
        return "search_general_web"
    else:
        return "search_certain_web"

# 2. Summarize documents
def summarize_documents(state: PipelineState) -> PipelineState:

    map_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒì˜ ê¸ˆìœµ ë¬¸ì„œë¥¼ ê¸ˆìœµ ì‹œì¥ ì „ë§, ì‹œí™©, í•µì‹¬ ì§€í‘œ ë° ìˆ˜ì¹˜, íˆ¬ì ì „ëµ ë° ì‹œì‚¬ì  ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. 
    ê¸ˆìœµ ë¬¸ì„œ:
    {context}
    """)
    reduce_template = ChatPromptTemplate.from_template("""
    ë‹¤ìŒì€ ì—¬ëŸ¬ ê¸ˆìœµ ë¬¸ì„œ ë¸”ë¡ë“¤ì˜ ì¤‘ê°„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ë³´ê³ ì„œì˜ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ìš”êµ¬ì‚¬í•­:
    - ì¤‘ë³µëœ í‘œí˜„ì€ í”¼í•˜ë˜, **ì¤‘ìš”í•œ ì •ë³´ëŠ” ìƒëµí•˜ì§€ ë§ê³  í¬í•¨**í•´ì£¼ì„¸ìš”.
    - ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ í•­ëª©ë³„ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
    - íŠ¹íˆ ì•„ë˜ 4ê°€ì§€ ì£¼ì œì— ë”°ë¼ êµ¬ë¶„í•˜ì—¬ í†µì°°ë ¥ ìˆê²Œ ê¸°ìˆ í•´ì£¼ì„¸ìš”:

    1. ì‹œì¥ ì „ë§ (ê²½ì œ ì „ì²´ì˜ ë°©í–¥, ê±°ì‹œ ì§€í‘œ í•´ì„ ë“±)
    2. ì‹œí™© ìš”ì•½ (ìµœê·¼ ì‹œì¥ ë™í–¥, ì£¼ìš” ë‰´ìŠ¤ ë° ì´ë²¤íŠ¸ ìš”ì•½)
    3. í•µì‹¬ ì§€í‘œ ë° ìˆ˜ì¹˜ (ê¸ˆë¦¬, í™˜ìœ¨, CPI ë“± ì¤‘ìš” ì§€í‘œ ë³€í™” í¬í•¨)
    4. íˆ¬ì ì „ëµ ë° ì‹œì‚¬ì  (íˆ¬ììì—ê²Œ ìœ ìš©í•œ ì „ëµ ë° ê´€ì°° í¬ì¸íŠ¸)

    ì¤‘ê°„ ìš”ì•½ ë‚´ìš©:
    {context}
    """)

    # map: sumarize each document
    print(state)
    map_chain = map_prompt | llm | StrOutputParser()
    mapped_summaries = [map_chain.invoke({"context": doc.page_content}) for doc in state["documents"]]
    # optionally log intermediate summaries 
    state["doc_summary_chunks"] = mapped_summaries
    print(mapped_summaries)
    # reduce: aggregate final summary 
    reduced_input = "\n\n".join(mapped_summaries)
    print(f'reduced_input is {reduced_input}')
    reduce_chain = reduce_template | llm | StrOutputParser()
    doc_summary = reduce_chain.invoke({"context": reduced_input})
    state["doc_summary"] = doc_summary

    filename = f"reports/middle_report_doc_summary_{state['date']}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# {state['date']} ë¦¬í¬íŠ¸\n\n{doc_summary}")
    print(f"[âœ“] {filename} ì €ì¥ ì™„ë£Œ")

    return state

# 3. Tavily Web Search for current insights
def search_general_web(state: PipelineState) -> PipelineState:

    search_query = "Today's global market performance by region US, Europe, Asia, and Korea; index, currency, oil, gold, central bank"
    tavily_tool = TavilySearchResults(k=5)
    results = tavily_tool.invoke({"query": search_query})
    web_results = "\n".join([r["content"] for r in results])

    prompt = f"""
    ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ì‹œì¥ ìš”ì•½ ìë£Œì…ë‹ˆë‹¤:

    {web_results}

    ê¸ˆìœµ ì‹œì¥ ì „ë§, ê¸€ë¡œë²Œ ì‹œí™©, ì£¼ìš” ì´ìŠˆ ë° ì˜í–¥ì— ëŒ€í•´ í†µì°°ë ¥ìˆê³ , ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    """
    summary = llm.invoke(prompt).content
    filename = f"reports/middle_report_general_search_{state['date']}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# {state['date']} ë¦¬í¬íŠ¸\n\n{summary}")
    print(f"[âœ“] {filename} ì €ì¥ ì™„ë£Œ")
    # ìš”ì•½ ì¶œë ¥
    print("\n[ìš”ì•½ ê²°ê³¼]")
    state["web_summary"] = summary
    return state 


def search_certain_web(state: PipelineState) -> PipelineState:
    
    search_query = f'{state["query"]}  ì‹œí™© {state["date"]}'
    tavily_tool = TavilySearchResults(k=2)
    results = tavily_tool.invoke({"query": search_query})
    
    prompt = f"""
    ë‹¤ìŒì€ '{search_query}'ì— ëŒ€í•œ ì›¹ ê¸°ì‚¬ ìš”ì•½ ìë£Œì…ë‹ˆë‹¤:
    {results}
    {search_query} ì „ë§, ì‹œí™©, ì£¼ìš” ì´ìŠˆ ë° ì˜í–¥ì— ëŒ€í•´ ìš”ì•½í•´ì£¼ì„¸ìš”.
    """
    summary = llm.invoke(prompt).content
    state["web_summary"] = summary
    # # ìš”ì•½ ì¶œë ¥
    # print("\n[ìš”ì•½ ê²°ê³¼]")
    # print(summary)
    filename = f"reports/middle_report_{search_query[:10]}_{state['date']}.md"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# {state['date']} ë¦¬í¬íŠ¸\n\n{summary}")
    print(f"[âœ“] {filename} ì €ì¥ ì™„ë£Œ")
    return state 


def merge_outputs(state: PipelineState) -> PipelineState:
    merge_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì´ˆë³´ íˆ¬ììë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì—ì„œ ë¶„ì„ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ëŠ” **ê¸ˆìœµ ë¶„ì„ê°€**ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ë³´ê³ ì„œ ê¸°ì¡´ ìš”ì•½ê³¼ ì‹¤ì‹œê°„ ê²€ìƒ‰ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ **ì¢…í•©ì ì´ê³  ì‰½ê³  ëª…í™•í•œ** í•œêµ­ íˆ¬ìì ì „ìš© ì¼ì¼ ê¸ˆìœµ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ê¸°ì¡´ ìš”ì•½:
{doc_summary}

ì‹¤ì‹œê°„ ê²€ìƒ‰ ìš”ì•½:
{web_summary}

âš ï¸ ì‘ì„± ì‹œ ìœ ì˜ì‚¬í•­:
- ì „ë¬¸ ìš©ì–´ëŠ” ê°€ëŠ¥í•œ ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê±°ë‚˜ ê´„í˜¸ ì•ˆì— ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.
- ìˆ«ìë‚˜ ì§€í‘œê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ ì˜ë¯¸ë¥¼ í•´ì„í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš” (ì˜ˆ: "ê¸ˆë¦¬ê°€ 3%ë¡œ ìƒìŠ¹ â†’ ëŒ€ì¶œ ì´ì ë¶€ë‹´ ì¦ê°€ë¡œ ì†Œë¹„ ìœ„ì¶• ê°€ëŠ¥ì„±").

ğŸ“Š ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

# ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë ˆí¬íŠ¸
                                                
### 1. ê¸ˆìœµ ì‹œì¥ ë™í–¥
- **ì£¼ìš” ì§€í‘œ ë° ì¶”ì„¸**: (ì˜ˆ: ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜ ìƒìŠ¹ë¥ , ì—ë„ˆì§€ ê°€ê²©, ê¸ˆë¦¬ ë³€í™” ë“±)
- **ì†Œë¹„ ì‹¬ë¦¬ ë¶„ì„**: ìµœê·¼ ì†Œë¹„ìë“¤ì´ ì–´ë–»ê²Œ ë°˜ì‘í•˜ê³  ìˆëŠ”ì§€
- **ì‹œì¥ ë™í–¥**: ì „ì²´ì ì¸ ì¦ì‹œ/ì±„ê¶Œ/í™˜ìœ¨ íë¦„ ìš”ì•½
                                                                                                
---

### 2. íˆ¬ì ì „ëµ ì‹œì‚¬ì 

 **ê¸°íšŒ ìš”ì¸ (Bullish signals)**  
- ì£¼ìš” ê¸°íšŒ ìš”ì¸ 1  
- ì£¼ìš” ê¸°íšŒ ìš”ì¸ 2   
- ì£¼ìš” ê¸°íšŒ ìš”ì¸ 3 

 **ë¦¬ìŠ¤í¬ ìš”ì¸ (Bearish signals)**  
- ì£¼ìš” ë¦¬ìŠ¤í¬ 1  
- ì£¼ìš” ë¦¬ìŠ¤í¬ 2   
- ì£¼ìš” ë¦¬ìŠ¤í¬ 3 

---

### 3. íˆ¬ì Action Point (í‘œ í˜•ì‹)

| êµ¬ë¶„        | íˆ¬ì ì•„ì´ë””ì–´                   | ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª… ë° ë¶„ì„ ê·¼ê±°              |
|-------------|------------------------------|-------------------------------------------|
| ì±„ê¶Œ        | êµ¬ì²´ì ì¸ ì±„ê¶Œ íˆ¬ì ì „ëµ        | ê¸ˆë¦¬ íë¦„, ì±„ê¶Œ ìˆ˜ìµë¥  ë“±ê³¼ ì—°ê´€ì§€ì–´ ì„¤ëª…   |
| ì£¼ì‹        | êµ¬ì²´ì ì¸ ì£¼ì‹ íˆ¬ì ì „ëµ        | ì‚°ì—… ë™í–¥, ë‰´ìŠ¤ ê¸°ë°˜ ì „ë§ í¬í•¨  |
| ì„¹í„° íšŒí”¼    | í”¼í•´ì•¼ í•  ì—…ì¢… ë˜ëŠ” í…Œë§ˆ        | ì•…ì¬ ìš”ì¸, ì‹¤ì  ë¶€ì§„ ë“± ì´ìœ  í¬í•¨      |
| ETF ì œì•ˆ    | ì¶”ì²œ ETF ë° ì´ìœ               | ì´ˆë³´ì ê¸°ì¤€ìœ¼ë¡œ ë¦¬ìŠ¤í¬/ë¶„ì‚° ì„¤ëª… í¬í•¨   |
| í™˜ìœ¨ ì „ëµ    | í™˜ìœ¨ê³¼ ê´€ë ¨ëœ ì „ëµ              | ìˆ˜ì¶œ/ìˆ˜ì… ê¸°ì—…ì— ì–´ë–¤ ì˜í–¥ì„ ì¤„ì§€ ì„¤ëª…   |

---

### 4.  ì „ëµì  ì œì–¸ (ì‰½ê²Œ ì •ë¦¬ëœ í¬íŠ¸í´ë¦¬ì˜¤ ì¡°ì–¸)
- **ì‹œì¥ ì „ë§ ìš”ì•½**: ì•ìœ¼ë¡œ ëª‡ ì£¼ê°„ ì–´ë–¤ íë¦„ì´ ì˜ˆìƒë˜ëŠ”ì§€
- **í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ë°©í–¥**: ì˜ˆ: "ì±„ê¶Œ 60% + ì£¼ì‹ 30% + ê¸ˆ 5% + ë¹„íŠ¸ì½”ì¸ 5% ë¹„ì¤‘ ì¶”ì²œ"
- **ì£¼ì˜í•´ì•¼ í•  ì **: ë¶ˆí™•ì‹¤ì„± ìš”ì¸, ì •ì±… ë°œí‘œ ì¼ì • ë“±
                                        
""")
    prompt_input = merge_prompt.format(
        doc_summary=state["doc_summary"],
        web_summary=state["web_summary"]
    )
    output = llm.invoke(prompt_input).content
    state["final_output"] = output
    return state

# 6. Save output to PDF and .md
def save_output(state: PipelineState) -> PipelineState:
    with open(f"reports/final_report_{state['date']}.md", "w", encoding="utf-8") as f:
        f.write(state["final_output"])
    return state

# LangGraph ì„¤ì •

builder = StateGraph(PipelineState)
# Register all nodes
builder.add_node("load_docs", load_docs)
builder.add_node("docs_summarize", summarize_documents)
builder.add_node("search_general_web", search_general_web)
builder.add_node("search_certain_web", search_certain_web)
builder.add_node("merge", merge_outputs)
builder.add_node("save", save_output)

# Flow control
builder.add_edge(START, "load_docs")
builder.add_edge("load_docs", "docs_summarize")
# builder.add_edge("docs_summarize", "check_query_type")

# Conditional routing based on query type
builder.add_conditional_edges(
    "docs_summarize",
    check_query_type,
    {
        "search_general_web": "search_general_web",
        "search_certain_web": "search_certain_web"
    }
)

# Common path after search
builder.add_edge("search_general_web", "merge")
builder.add_edge("search_certain_web", "merge")
builder.add_edge("merge", "save")
builder.add_edge("save", END)

# Compile app
adaptive_rag = builder.compile()


if __name__ == "__main__":

    # Initialize state with required keys
    inputs = {
        "date": date.today().isoformat(),
        "query": "ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”",
        "documents": [],  # Initialize empty documents list
        "doc_summary_chunks": [],
        "doc_summary": "",
        "web_summary": "",
        "final_output": "",
        "db_path": SUMMARY_CACHE_PATH
    }

    final_output = adaptive_rag.invoke(inputs)


    # Visualize graph
    # png_graph = adaptive_rag.get_graph().draw_mermaid_png()
    png_graph = adaptive_rag.get_graph().draw_mermaid_png(
        draw_method=MermaidDrawMethod.PYPPETEER
    )

    with open("graph.png", "wb") as f:
        f.write(png_graph)



